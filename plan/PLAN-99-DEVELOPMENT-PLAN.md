# PUnit Development Plan

This document is the source of truth for PUnit's phased development. It consolidates the core probabilistic testing framework and the experiment extension into a single comprehensive plan.

---

## Overview

PUnit development is organized into three major tracks:

| Track | Description | Phases |
|-------|-------------|--------|
| **Core Framework** | Foundational `@ProbabilisticTest` infrastructure | C1‚ÄìC8 |
| **Experiment Extension** | `@Experiment`, baselines, specifications | E1‚ÄìE8 |
| **Enhancements** | Statistical improvements, prompt refinement, tooling | A, B, C, D |

---

## Track 1: Core Probabilistic Testing Framework (C1‚ÄìC8)

These phases establish the foundational `@ProbabilisticTest` annotation and supporting infrastructure.

### Phase C1: Core Framework (MVP)

**Status**: ‚úÖ Complete

**Deliverables**:
1. `@ProbabilisticTest` annotation with `samples` and `minPassRate`
2. `ProbabilisticTestExtension` implementing `TestTemplateInvocationContextProvider`
3. `SampleResultAggregator` with basic pass/fail counting
4. `InvocationInterceptor` to catch and record assertion failures
5. `FinalVerdictDecider` with simple pass rate comparison
6. Basic failure message with statistics
7. `TestReporter` integration for structured output

**Estimated Effort**: 2-3 days

---

### Phase C2: Early Termination

**Status**: ‚úÖ Complete

**Deliverables**:
1. `EarlyTerminationEvaluator` with impossibility check
2. Stream short-circuiting in invocation provider
3. `TerminationReason` enum and reporting integration
4. Update failure message to include termination cause

**Estimated Effort**: 1-2 days

---

### Phase C3: Configuration System

**Status**: ‚úÖ Complete

**Deliverables**:
1. `ConfigurationResolver` with precedence logic
2. System property support (`punit.samples`, `punit.minPassRate`)
3. Environment variable support
4. `punit.samplesMultiplier` for scaling
5. Validation and error messages for invalid configurations

**Estimated Effort**: 1 day

---

### Phase C4: Method-Level Cost Budget & Time Limits

**Status**: ‚úÖ Complete

**Deliverables**:
1. `CostBudgetMonitor` implementation (wall-clock time + token accumulation)
2. `timeBudgetMs` annotation parameter
3. `tokenCharge` and `tokenBudget` annotation parameters
4. `TokenChargeRecorder` interface and `DefaultTokenChargeRecorder` implementation
5. JUnit 5 `ParameterResolver` for injecting `TokenChargeRecorder` into test methods
6. `onBudgetExhausted` behavior options
7. Static mode: Pre-sample budget checks integrated into sample loop
8. Dynamic mode: Post-sample token accumulation with recorder
9. Mode detection logic (check for TokenChargeRecorder parameter)
10. Reporting integration for budget-related termination

**Estimated Effort**: 3-4 days

---

### Phase C5: Budget Scopes (Class and Suite Levels)

**Status**: ‚úÖ Complete

**Deliverables**:
1. `@ProbabilisticTestBudget` class-level annotation
2. `ProbabilisticTestBudgetExtension` for class-level budget management
3. `SharedBudgetMonitor` thread-safe implementation for shared budgets
4. `SuiteBudgetManager` singleton for suite-level state
5. Suite-level system property parsing (`punit.suite.*`)
6. Budget scope precedence logic (suite ‚Üí class ‚Üí method)
7. Consumption propagation to all active scopes
8. Extended `TerminationReason` enum with scoped values
9. Reporting integration for scoped budgets
10. Thread-safe budget tracking for parallel test execution

**Estimated Effort**: 3-4 days

---

### Phase C6: Enhanced Reporting & Diagnostics

**Status**: ‚úÖ Complete

**Deliverables**:
1. `maxExampleFailures` parameter
2. Example failure collection in aggregator
3. Suppressed exceptions on final `AssertionError`
4. Enhanced failure message with example failures
5. Per-sample display names (e.g., "Sample 3/100")

**Estimated Effort**: 1 day

---

### Phase C7: Exception Handling & Edge Cases

**Status**: ‚úÖ Complete

**Deliverables**:
1. `onException` parameter and handling logic
2. Parameter validation at discovery time
3. Edge case handling (samples=1, minPassRate=1.0, etc.)
4. Documentation of rounding behavior

**Estimated Effort**: 1 day

---

### Phase C8: Documentation & Polish

**Status**: ‚úÖ Complete

**Deliverables**:
1. Javadoc for all public API
2. README with quick start guide
3. Examples in test sources
4. Migration guide (for teams adopting PUnit)

**Estimated Effort**: 1-2 days

---

**Track 1 Total**: 13-21 days (Complete)

---

## Track 2: Experiment Extension (E1‚ÄìE8)

These phases extend PUnit with experiment support, baselines, and specifications.

### Phase E1: Core Use Case and Result Abstractions

**Status**: ‚úÖ Complete

**Goals**: Establish foundational abstractions for use cases and results

**Deliverables**:
- `@UseCase` annotation
- `UseCaseResult` class with builder pattern
- `UseCaseContext` interface
- `UseCaseRegistry` for discovery and caching

**Dependencies**: None (greenfield)

**Estimated Effort**: 2-3 days

---

### Phase E2: Single-Config Experiment Mode

**Status**: ‚úÖ Complete

**Goals**: Enable exploratory execution via `@Experiment`

**Deliverables**:
- `@Experiment` and `@ExperimentContext` annotations
- `ExperimentExtension` (JUnit 5 extension)
- `ExperimentResultAggregator`
- `EmpiricalBaselineGenerator`
- YAML serialization/deserialization

**Dependencies**: Phase E1

**Estimated Effort**: 4-5 days

---

### Phase E2b: Multi-Config Experiments

**Status**: ‚úÖ Complete

**Goals**: Enable experiments with explicit `ExperimentConfig` lists

**Deliverables**:
- `@ExperimentDesign`, `@Config`, `@ExperimentGoal` annotations
- Sequential config execution with shared budget
- Goal-based early termination
- Aggregated `SUMMARY.yaml` report generation

**Dependencies**: Phase E2

**Estimated Effort**: 4-5 days

---

### Phase E2c: Adaptive Experiments

**Status**: ‚úÖ Complete

**Goals**: Enable dynamic level generation for adaptive factors

**Deliverables**:
- `AdaptiveFactor<T>` interface
- `AdaptiveLevels` builder with `Supplier` support
- `RefinementStrategy<T>` SPI interface
- `IterationFeedback` and `FailureObservation` models
- Adaptive baseline generation with iteration history

**Dependencies**: Phase E2b

**Estimated Effort**: 5-6 days

---

### Phase E3: Specification Representation and Registry

**Status**: ‚úÖ Complete

**Goals**: Define specification data model and resolution

**Deliverables**:
- `ExecutionSpecification` model class
- `SpecificationRegistry` for loading and resolving specs
- YAML format for specifications
- `SuccessCriteria` expression parsing

**Dependencies**: Phase E1

**Estimated Effort**: 3-4 days

---

### Phase E4: Spec-Driven Probabilistic Tests

**Status**: ‚úÖ Complete

**Goals**: Extend `@ProbabilisticTest` to consume specifications

**Deliverables**:
- Add `spec` and `useCase` attributes
- `SuccessCriteria` evaluation integration
- Baseline-spec drift detection
- Provenance reporting

**Dependencies**: Phases E1, E2, E3

**Estimated Effort**: 4-5 days

---

### Phase E5: Pluggable Backend Infrastructure

**Status**: ‚úÖ Complete

**Goals**: Establish SPI for experiment backends

**Deliverables**:
- `ExperimentBackend` SPI interface
- `ExperimentBackendRegistry` with ServiceLoader discovery
- Generic backend implementation

**Dependencies**: Phase E2

**Estimated Effort**: 2-3 days

---

### Phase E6: LLM Backend Extension (llmx)

**Status**: ‚úÖ Complete

**Goals**: Implement LLM-specific backend as reference implementation

**Deliverables**:
- `org.javai.punit.llmx` package
- `LlmExperimentBackend`, `LlmUseCaseContext`
- Common model/temperature presets
- `LlmPromptRefinementStrategy` for adaptive experiments

**Dependencies**: Phases E5, E2c

**Estimated Effort**: 4-5 days

---

### Phase E7: Canonical Flow Examples

**Status**: ‚úÖ Complete

**Goals**: Demonstrate complete canonical flow through working examples

**Deliverables**:
- End-to-end examples in test suite
- Single-config, multi-config, and adaptive experiment examples
- Mock LLM client for testing

**Dependencies**: Phases E4, E6, E2c

**Estimated Effort**: 3-4 days

---

### Phase E8: Documentation, Migration, and Guardrails

**Status**: ‚úÖ Complete

**Goals**: Complete documentation and governance mechanisms

**Deliverables**:
- Javadoc for all public APIs
- User guide and migration guide
- Complete governance warnings/errors
- Edge case testing

**Dependencies**: Phases E1-E7

**Estimated Effort**: 3-4 days

---

**Track 2 Total**: 35-44 days (Complete)

---

## Track 3: Enhancements (Phases A, B, C)

These phases introduce advanced capabilities for cost tracking, prompt refinement, and statistical threshold derivation.

### Phase A: Core Statistical and Cost Enhancements

**Status**: üìã Planned

**Priority**: P1

**Goals**: Improve cost tracking and statistical guidance

**Deliverables**:

#### A.1 Input/Output Token Split

Enhanced `CostSummary` record:

```java
public record CostSummary(
    long totalTimeMs,
    long avgTimePerSampleMs,
    long totalTokens,
    long avgTokensPerSample,
    long totalInputTokens,          // NEW
    long totalOutputTokens,         // NEW
    long avgInputTokensPerSample,   // NEW
    long avgOutputTokensPerSample,  // NEW
    long apiCallCount               // NEW
) {}
```

#### A.2 Token Estimation

```java
public interface TokenEstimator {
    long estimateInputTokens(String text);
    long estimateOutputTokens(String text);
    
    static TokenEstimator forModel(String modelName) { ... }
}
```

With `BasicCostEstimator` fallback using word-count heuristics (~1.3 tokens per word).

#### A.3 Sample Size Advisory

Baseline output includes guidance on samples needed for various precision levels.

#### A.4 Stability-Based Early Termination

```java
@Experiment(
    useCase = "usecase.json.generation",
    samples = 1000,
    stabilityThreshold = 0.02  // Stop when CI width < 2%
)
```

**Dependencies**: Phase E2

**Estimated Effort**: 3-4 days

---

### Phase B: Adaptive Prompt Refinement

**Status**: üìã Planned

**Priority**: P2

**Goals**: Enable automated prompt improvement through LLM-assisted refinement

**Deliverables**:

#### B.1 PromptContributor Interface

```java
public interface PromptContributor {
    String getSystemMessage();
    default List<Example> getExamples() { return List.of(); }
    default Optional<String> getUserMessageTemplate() { return Optional.empty(); }
    
    record Example(String userMessage, String assistantResponse) {}
}
```

#### B.2 Failure Categorization SPI

```java
@FunctionalInterface
public interface FailureCategorizer {
    String categorize(UseCaseResult result);
}
```

#### B.3 @AdaptivePromptExperiment Annotation

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface AdaptivePromptExperiment {
    Class<? extends PromptContributor> promptContributor();
    int samplesPerIteration() default 50;
    int maxIterations() default 10;
    double targetSuccessRate() default Double.NaN;
    Class<? extends FailureCategorizer> failureCategorizer();
    String refinementModel() default "";
    String refinementPromptTemplate() default "";
    String[] refinableComponents() default {};
}
```

#### B.4 Refinement Loop Orchestration

1. Call PromptContributor for initial components
2. Run N samples, collect results
3. Apply failure categorization
4. Check goal; if not met, refine via LLM
5. Loop until goal met or limit reached

**Dependencies**: Phases E2c, E6, A

**Estimated Effort**: 5-6 days

---

### Phase C: Pass Rate Threshold Derivation

**Status**: üìã Planned

**Priority**: P1

**Goals**: Enable statistically-derived pass rate thresholds for probabilistic tests

**Deliverables**:

#### C.1 Problem Statement

An experiment runs 1000 samples and observes 95.1% success rate. If the regression test runs only 100 samples with a 95.1% threshold, normal sampling variance will cause false failures.

**Solution**: Derive a one-sided lower confidence bound that accounts for increased variance in smaller test samples.

#### C.2 punit-statistics Module

Per Design Principle 1.6, an isolated module with:
- No dependencies on punit-core or punit-experiment
- Only Java standard library
- Comprehensive unit tests with worked examples
- Code readable by professional statisticians

#### C.3 Wilson Score Lower Bound

```
p_lower = (pÃÇ + z¬≤/2n - z‚àö(pÃÇ(1-pÃÇ)/n + z¬≤/4n¬≤)) / (1 + z¬≤/n)
```

Preferred for small samples or extreme success rates.

#### C.4 RegressionThreshold and Calculator

```java
public record RegressionThreshold(
    ExperimentalBasis basis,
    TestConfiguration testConfig,
    double minPassRate,
    DerivationMetadata derivation
) {}

public class RegressionThresholdCalculator {
    public RegressionThreshold calculate(
            int experimentSamples,
            int experimentSuccesses,
            int testSamples,
            double confidenceLevel) { ... }
}
```

#### C.5 Three Operational Approaches

| Approach | Given | Framework Computes |
|----------|-------|-------------------|
| **Sample-Size-First** (Cost-Driven) | Samples + Confidence | Threshold |
| **Confidence-First** (Quality-Driven) | Confidence + Effect + Power | Sample Size |
| **Threshold-First** (Baseline-Anchored) | Samples + Explicit Threshold | Implied Confidence |

#### C.6 Enhanced @ProbabilisticTest

```java
@ProbabilisticTest(
    spec = "my-spec:v1",
    thresholdConfidence = 0.95,
    derivationPolicy = ThresholdDerivationPolicy.DERIVE
)
```

#### C.7 Qualified Failure Reporting

Reports include:
- Observed pass rate vs. threshold
- Confidence level and statistical method used
- Probability of false positive

**Dependencies**: Phases E2, E3, E4

**Estimated Effort**: 6-8 days

---

---

### Phase D: Baseline Review and Approval Workflow

**Status**: üìã Planned

**Priority**: P1

**Goals**: Provide Gradle tasks for reviewing baselines and creating specifications

**Deliverables**:

#### D.1 `punitReview` Task

Display baseline information with computed statistics:

```bash
./gradlew punitReview --useCase=json.generation     # Display baseline
./gradlew punitReview --list-pending                # List unapproved baselines
```

Features:
- Display current baseline with raw data (samples, successes, rate)
- Compute and display 95% confidence interval on-the-fly
- Show history of previous baseline runs
- Compare current to previous baselines
- Identify pending approvals (baselines without specs)

#### D.2 `punitApprove` Task

Create specification from baseline (non-interactive):

```bash
./gradlew punitApprove --useCase=json.generation
./gradlew punitApprove --useCase=json.generation --notes="Reviewed after fix"
./gradlew punitApprove --useCase=json.generation --force  # New version
```

Features:
- Non-interactive (no stdin prompts) for CI/CD compatibility
- Auto-capture approval metadata (timestamp, user)
- Auto-increment version if spec exists
- Validate baseline exists before approval

#### D.3 Baseline History

Baseline files include history of previous runs:

```yaml
history:
  - generatedAt: 2026-01-05T10:00:00Z
    samples: 1000
    successes: 912
    observedRate: 0.912
  - generatedAt: 2026-01-03T09:15:00Z
    samples: 500
    successes: 471
    observedRate: 0.942
```

Features:
- Auto-append on each experiment run
- Configurable history limit (default 10)
- Enables trend analysis at review time

**Dependencies**: Phase E2

**Estimated Effort**: 3-4 days

---

**Track 3 Total**: 17-22 days (Planned)

---

## Phase Summary

| Track | Phase | Description | Status | Est. Days |
|-------|-------|-------------|--------|-----------|
| Core | C1 | Core Framework (MVP) | ‚úÖ | 2-3 |
| Core | C2 | Early Termination | ‚úÖ | 1-2 |
| Core | C3 | Configuration System | ‚úÖ | 1 |
| Core | C4 | Method-Level Cost Budget | ‚úÖ | 3-4 |
| Core | C5 | Budget Scopes | ‚úÖ | 3-4 |
| Core | C6 | Enhanced Reporting | ‚úÖ | 1 |
| Core | C7 | Exception Handling | ‚úÖ | 1 |
| Core | C8 | Documentation | ‚úÖ | 1-2 |
| Experiment | E1 | Use Case Abstractions | ‚úÖ | 2-3 |
| Experiment | E2 | Single-Config Experiments | ‚úÖ | 4-5 |
| Experiment | E2b | Multi-Config Experiments | ‚úÖ | 4-5 |
| Experiment | E2c | Adaptive Experiments | ‚úÖ | 5-6 |
| Experiment | E3 | Specification Registry | ‚úÖ | 3-4 |
| Experiment | E4 | Spec-Driven Tests | ‚úÖ | 4-5 |
| Experiment | E5 | Backend Infrastructure | ‚úÖ | 2-3 |
| Experiment | E6 | LLM Backend (llmx) | ‚úÖ | 4-5 |
| Experiment | E7 | Canonical Flow Examples | ‚úÖ | 3-4 |
| Experiment | E8 | Documentation | ‚úÖ | 3-4 |
| Enhancement | A | Cost/Statistical Enhancements | üìã | 3-4 |
| Enhancement | B | Adaptive Prompt Refinement | üìã | 5-6 |
| Enhancement | C | Threshold Derivation | üìã | 6-8 |
| Enhancement | D | Review & Approval Workflow | üìã | 3-4 |

**Total Estimated Effort**: 65-87 days

---

## Recommended Execution Order

1. ‚úÖ Complete Phases C1-C8 (Core Framework)
2. ‚úÖ Complete Phases E1-E8 (Experiment Extension)
3. üìã Phase D (review & approval workflow) ‚Äî enables practical usage
4. üìã Phase C (isolated statistics module + threshold derivation)
5. üìã Phase A (cost/statistical enhancements)
6. üìã Phase B (prompt refinement)

---

## Explicit Exclusions

| Feature | Reason |
|---------|--------|
| Dollar cost calculations | Framework provides tokens/time only |
| Automated config optimization | Decisions remain with humans |
| Cross-experiment state management | Each experiment is independent |
| Visualization/dashboards | Results via JUnit only |
| Retry orchestration | Application's responsibility |

---

## v1 Scope vs v2 Enhancements

### v1 Scope (Complete)

‚úÖ `@ProbabilisticTest` annotation  
‚úÖ `samples` and `minPassRate` parameters  
‚úÖ Sequential sample execution  
‚úÖ Early termination by impossibility  
‚úÖ Wall-clock time budget (`timeBudgetMs`)  
‚úÖ Static token budget (`tokenCharge`, `tokenBudget`)  
‚úÖ Dynamic token charging via injectable `TokenChargeRecorder`  
‚úÖ Budget scopes: method, class (`@ProbabilisticTestBudget`), and suite levels  
‚úÖ System property / env var overrides (including suite-level budgets)  
‚úÖ Structured reporting via `TestReporter`  
‚úÖ Clear failure messages with statistics  
‚úÖ Example failure capture  
‚úÖ Basic exception handling policy  
‚úÖ `@Experiment` and experiment infrastructure  
‚úÖ `@UseCase` and use case abstractions  
‚úÖ Empirical baselines and specifications  
‚úÖ Backend SPI and `llmx` extension

### v2 Enhancements (Future)

‚è≥ **Parallel sample execution** (`parallelSamples` parameter)  
‚è≥ **Custom cost models** (SPI for money, API calls, dynamic token measurement)  
‚è≥ **Warm-up samples** (excluded from statistics)  
‚è≥ **Per-invocation timeout** (distinct from overall budget)  
‚è≥ **Retry policy** (retry failed samples N times before counting as failure)  
‚è≥ **Combined with @ParameterizedTest** (run probabilistic test for each parameter set)  
‚è≥ **Statistical confidence intervals** (report confidence bounds, not just point estimate)  
‚è≥ **Adaptive sampling** (stop early when statistically confident of pass/fail)  
‚è≥ **JUnit XML/HTML report integration** (custom reporter for richer output)  
‚è≥ **Flakiness tracking** (track pass rates over time across runs)  
‚è≥ **Gradle/Maven plugin** (configure defaults in build file)  
‚è≥ **Automatic token measurement** (SPI for intercepting LLM client calls)

---

*See [PLAN-99-DEVELOPMENT-STATUS.md](./PLAN-99-DEVELOPMENT-STATUS.md) for current implementation status.*

*[Back to Table of Contents](./DOC-00-TOC.md)*

