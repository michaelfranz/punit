# PUnit User Guide

The complete guide to probabilistic testing with PUnit.

This document walks you through the canonical operational flow—from defining use cases, through experimentation, to running statistically-grounded tests in CI.

---

## Overview

PUnit follows a streamlined workflow:

```
Use Case → Experiment → Spec (commit) → Probabilistic Test
```

1. **Use Case**: Define *what* behavior you want to observe
2. **Experiment**: Run the use case repeatedly to gather empirical data  
3. **Spec**: Machine-generated summary of observed behavior (commit to Git)
4. **Probabilistic Test**: CI-gated test using spec-derived thresholds

---

## Step 1: Create a Use Case

A **Use Case** is a reusable method that invokes production code and captures observations. It returns a neutral `UseCaseResult` containing key-value pairs.

### Example: JSON Validation Use Case

```java
package com.example.usecases;

import org.javai.punit.experiment.api.UseCase;
import org.javai.punit.experiment.api.UseCaseContext;
import org.javai.punit.experiment.model.UseCaseResult;

public class JsonGenerationUseCases {

    // Inject your production service
    private final LlmClient llmClient;

    public JsonGenerationUseCases(LlmClient llmClient) {
        this.llmClient = llmClient;
    }

    @UseCase("usecase.json.generation")
    public UseCaseResult generateJson(String prompt, UseCaseContext context) {
        // 1. Get configuration from context (optional)
        String model = context.getParameter("model", String.class, "gpt-4");
        double temperature = context.getParameter("temperature", Double.class, 0.7);

        // 2. Invoke production code
        LlmResponse response = llmClient.complete(prompt, model, temperature);

        // 3. Capture observations as key-value pairs
        boolean isValidJson = JsonValidator.isValid(response.getContent());
        
        return UseCaseResult.builder()
            .value("isValidJson", isValidJson)
            .value("content", response.getContent())
            .value("tokensUsed", response.getTokensUsed())
            .meta("model", model)
            .meta("requestId", response.getRequestId())
            .build();
    }
}
```

### Key Points

- **Annotate with `@UseCase("id")`**: Use a dot-separated namespace (e.g., `usecase.json.generation`)
- **Accept `UseCaseContext`**: Provides backend-specific parameters
- **Return `UseCaseResult`**: Neutral observations, no assertions
- **Never called by production code**: Use cases exist only in test/experiment space

---

## Step 2: Create an Experiment

An **Experiment** executes a use case repeatedly to gather empirical data. Experiments are exploratory—they never produce pass/fail verdicts.

### Experiment Modes

| Mode | Purpose | Output |
|------|---------|--------|
| `MEASURE` | Establish reliable statistics (1000+ samples) | `specs/{UseCaseId}.yaml` |
| `EXPLORE` | Compare configurations (1-10 samples each) | `explorations/{UseCaseId}/*.yaml` |

### Example: MEASURE Experiment

```java
package com.example.experiments;

import org.javai.punit.api.UseCaseProvider;
import org.javai.punit.experiment.api.Experiment;
import org.javai.punit.experiment.api.ExperimentMode;
import org.javai.punit.experiment.api.ResultCaptor;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.extension.RegisterExtension;

public class JsonGenerationExperiment {

    @RegisterExtension
    UseCaseProvider provider = new UseCaseProvider();

    @BeforeEach
    void setUp() {
        provider.register(JsonGenerationUseCase.class, () ->
            new JsonGenerationUseCase(new OpenAIClient("gpt-4", 0.7))
        );
    }

    @Experiment(
        mode = ExperimentMode.MEASURE,   // Mandatory: MEASURE or EXPLORE
        useCase = JsonGenerationUseCase.class,
        samples = 1000,                   // Default for MEASURE; 0 = use mode default
        tokenBudget = 500000,             // Stop if tokens exceed 500k
        timeBudgetMs = 600000             // Stop after 10 minutes
    )
    void measureJsonGenerationPerformance(JsonGenerationUseCase useCase, ResultCaptor captor) {
        captor.record(useCase.generateJson("Create a user profile"));
    }
}
```

### Run the Experiment

```bash
# For MEASURE experiments (generate specs for tests)
./gradlew measure --tests "JsonGenerationExperiment"

# For EXPLORE experiments (compare configurations)
./gradlew explore --tests "JsonGenerationExperiment.exploreModels"
```

### Generated Spec

After running, PUnit generates a spec directly at:

```
src/test/resources/punit/specs/usecase.json.generation.yaml
```

Example spec content:

```yaml
# Execution Specification for usecase.json.generation
# Generated by PUnit MEASURE experiment

schemaVersion: punit-spec-2
specId: usecase.json.generation
useCaseId: usecase.json.generation
generatedAt: 2026-01-09T15:30:00Z

# Core empirical data (used for threshold derivation)
empiricalBasis:
  samples: 1000
  successes: 935
  generatedAt: 2026-01-09T15:30:00Z

# Extended statistics (for analysis)
extendedStatistics:
  standardError: 0.0078
  confidenceInterval:
    lower: 0.919
    upper: 0.949
  failureDistribution:
    malformed_json: 35
    empty_response: 20
    timeout: 10
  totalTimeMs: 450000
  avgTimePerSampleMs: 450

# Integrity check
contentFingerprint: sha256:abc123...
```

### Key Points

- **`mode` is mandatory**: Choose `MEASURE` or `EXPLORE`
- **`samples`**: How many times to run (0 = use mode's default: 1000 for MEASURE, 1 for EXPLORE)
- **`tokenBudget` / `timeBudgetMs`**: Resource limits
- **`UseCaseProvider`**: Configures how use cases are instantiated
- **`ResultCaptor`**: Records results for aggregation into specs
- **Specs are descriptive**: They record what happened; thresholds are derived at runtime

---

## Step 3: Commit the Spec

After running a MEASURE experiment:

1. **Review** the generated spec: `src/test/resources/punit/specs/{UseCaseId}.yaml`
2. **Commit** to version control: `git add . && git commit -m "Add spec for JsonGenerationUseCase"`
3. (Optional) Use a **Pull Request** for team review

That's it! No separate approval step—approval is your commit.

---

## Step 4: Create a Probabilistic Test

A **Probabilistic Test** uses the spec's empirical data to derive thresholds at runtime.

### Spec-Driven Test (Recommended)

```java
package com.example.tests;

import org.javai.punit.api.ProbabilisticTest;
import org.javai.punit.api.TokenChargeRecorder;
import static org.assertj.core.api.Assertions.assertThat;

class JsonGenerationTest {

    private final LlmClient llmClient = new LlmClient();

    @ProbabilisticTest(
        useCase = JsonGenerationUseCase.class,  // Spec looked up by use case ID
        samples = 50                             // Run 50 times
        // minPassRate derived from spec's empiricalBasis (95% CI lower bound)
    )
    void shouldGenerateValidJson(TokenChargeRecorder tokenRecorder) {
        LlmResponse response = llmClient.complete(
            "Generate a JSON object with name and age",
            "gpt-4",
            0.7
        );
        tokenRecorder.recordTokens(response.getUsage().getTotalTokens());
        
        assertThat(JsonValidator.isValid(response.getContent())).isTrue();
    }
}
```

### Inline Threshold (Simple/Fallback)

When you don't have a spec or want to override:

```java
@ProbabilisticTest(
    samples = 50,
    minPassRate = 0.90  // Explicit threshold (no spec lookup)
)
void shouldGenerateValidJsonExplicit(TokenChargeRecorder tokenRecorder) {
    LlmResponse response = llmClient.complete("Generate JSON...", "gpt-4", 0.7);
    tokenRecorder.recordTokens(response.getUsage().getTotalTokens());
    assertThat(JsonValidator.isValid(response.getContent())).isTrue();
}
```

### Run Tests

```bash
./gradlew test
```

### Understanding Results

```
✅ shouldGenerateValidJson()
    ✅ Sample 1/50
    ✅ Sample 2/50
    ...
    ✅ Sample 45/50  ← SUCCESS_GUARANTEED, remaining samples skipped
    
Probabilistic test passed: 100.00% >= 91.90% (45/45 samples succeeded)
```

Or if it fails:

```
❌ shouldGenerateValidJson()
    ✅ Sample 1/50
    ❌ Sample 2/50  ← Assertion failed
    ...
    ❌ Sample 10/50  ← IMPOSSIBILITY: cannot reach 91.9% pass rate
    
Probabilistic test failed: observed pass rate 72.00% < required 91.90%
```

---

## Summary

| Step | Command | Output |
|------|---------|--------|
| 1. Define Use Case | — | `@UseCase` class |
| 2. Run Experiment | `./gradlew measure --tests "..."` | `specs/{UseCaseId}.yaml` |
| 3. Commit Spec | `git commit` | Version-controlled spec |
| 4. Run Tests | `./gradlew test` | CI pass/fail |

---

## Quick Reference

### Choosing the Right Mode

| Question | Mode | Samples |
|----------|------|---------|
| "What's the true success rate?" | `MEASURE` | 1000+ |
| "Which config is best?" | `EXPLORE` | 1-10 per config |

### Gradle Commands

```bash
# MEASURE: Generate specs for probabilistic tests
./gradlew measure --tests "MyExperiment.measureSomething"

# EXPLORE: Compare configurations
./gradlew explore --tests "MyExperiment.exploreConfigurations"

# Run probabilistic tests
./gradlew test
```

### Budget Control

```java
@ProbabilisticTest(
    useCase = MyUseCase.class,
    samples = 100,
    timeBudgetMs = 30000,     // Stop after 30 seconds
    tokenBudget = 50000,      // Stop after 50k tokens
    onBudgetExhausted = BudgetExhaustedBehavior.EVALUATE_PARTIAL
)
```

### Class-Level Shared Budget

```java
@ProbabilisticTestBudget(tokenBudget = 100000, timeBudgetMs = 60000)
class LlmIntegrationTests {
    
    @ProbabilisticTest(useCase = MyUseCase.class, samples = 30)
    void test1(TokenChargeRecorder recorder) { /* ... */ }
    
    @ProbabilisticTest(useCase = MyUseCase.class, samples = 30)
    void test2(TokenChargeRecorder recorder) { /* ... */ }
}
```

---

## Next Steps

- See [README.md](../README.md) for full configuration reference
- See [OPERATIONAL-FLOW.md](OPERATIONAL-FLOW.md) for detailed workflow documentation
- Explore examples in `src/test/java/org/javai/punit/examples/`
